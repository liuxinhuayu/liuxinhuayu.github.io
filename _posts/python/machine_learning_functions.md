1、熵、相对熵、交叉熵的计算推导

事件A发生概率：p(A)
信息量：log(1/p(A))=-log(p(A))

熵（所有信息量的期望）：-sum(p(xi)*log(p(xi)))    i=1,2...n,即有n种事件发生的可能
当n=2时，即事件只有2种可能，为二项分布，熵的计算方法可简化：
   H(X)
= -sum(p(xi)*log(p(xi)))
= -( p(x1)*log(p(x1))+p(x2)*log(p(x2)) )
= -( p(x1)*log(p(x1))+(1-p(x1))*log(1-p(x1)) )

相对熵（KL散度）：同一个随机变量，若有2个单独的概率分布p(x),q(x),可以使用KL散度衡量两个分布的差异。
D(p||q) = sum( p(xi)log(p(xi)/q(xi)) )    i=1,2...n,即有n种事件发生的可能
在机器学习中，p表示样本真实分布（[1,0,0]表示属于第1类），q表示模型预测分布（如[0.7,0.2,0.1]）
D值越小，表示p,q越相近，模型预测越准确。

  D(p||q) 
= sum( p(xi)log(p(xi)/q(xi)) )
= sum(p(xi)log(p(xi))) - sum(p(xi)log(q(xi)))
= - H(X) + ( -sum(p(xi)log(q(xi))) )
第一部分为熵，第二部分为交叉熵： -sum(p(xi)log(q(xi))) 
由于H(X)不变，只需关注交叉熵即可，因此一般在机器学习中使用交叉熵做损失函数来评估模型。


交叉熵的应用：

0）线性回归问题
用MSE(Mean Squared Error)作为loss函数：
loss = 1/n * sum((yi-yi')^2)

1）单分类问题--每个样本只有一个类别，一个样本在所有类别下预测的概率之和为1
loss = -sum( yi*log(yi') )    i=0,1,2...m 表示有m个类别
yi为样本真实类别，yi'为模型预测类别。
以上为1个样本的损失函数，n个样本即求均值。

2）多分类问题--每个样本有多个类别，一个样本在各类别下预测的概率独立不相关，其加和不一定为1
1个样本的损失函数:
loss = sum(loss(yi))     i=0,1,2...m 表示有m个类别
其中yi表示第i类发生的概率，每一类发生的可能都是二项分布：
  loss(yi) 
= -sum( yi*log(yi') ) 
= - yi*log(yi') - (1-yi)*log((1-yi'))


2、L1范数、L2范数

3、sigmoid函数

4、softmax函数

5、朴素贝叶斯
